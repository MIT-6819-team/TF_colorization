{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GPU_NUMBER = 3\n",
    "root = '/data/vision/torralba/health-habits/other/enes/'\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "sys.path.append( root + 'Utils/')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from pprint import pprint\n",
    "from notebook_utils import *\n",
    "from skimage import color, io\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_NUMBER);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, s):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, s, s, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_tensorflow_graph(BATCH_SIZE):\n",
    "  \n",
    "  image_ = tf.placeholder( tf.float32, shape = [BATCH_SIZE, 256, 256, 1] )\n",
    "  output_ = tf.placeholder( tf.float32, shape = [BATCH_SIZE, 64, 64, 313])\n",
    "\n",
    "  W1_1 = weight_variable([3,3,1,64])\n",
    "  b1_1 = bias_variable([64])\n",
    "  conv1_1 = conv2d( image_, W1_1, 1 ) + b1_1\n",
    "  conv1_1 = tf.nn.relu(conv1_1)\n",
    "\n",
    "  W1_2 = weight_variable([3,3,64,64])\n",
    "  b1_2 = bias_variable([64])\n",
    "  conv1_2 = conv2d( conv1_1, W1_2, 2 ) + b1_2\n",
    "  conv1_2 = tf.nn.relu(conv1_2)\n",
    "\n",
    "  conv1_2 = tf.contrib.layers.batch_norm(conv1_2)\n",
    "\n",
    "  W2_1 = weight_variable([3,3,64,128])\n",
    "  b2_1 = bias_variable([128])\n",
    "  conv2_1 = conv2d( conv1_2, W2_1, 1 ) + b2_1\n",
    "  conv2_1 = tf.nn.relu(conv2_1)\n",
    "\n",
    "  W2_2 = weight_variable([3,3,128,128])\n",
    "  b2_2 = bias_variable([128])\n",
    "  conv2_2 = conv2d( conv2_1, W2_2, 2 ) + b2_2\n",
    "  conv2_2 = tf.nn.relu(conv2_2)\n",
    "\n",
    "  conv2_2 = tf.contrib.layers.batch_norm(conv2_2)\n",
    "\n",
    "  W3_1 = weight_variable([3,3,128,256])\n",
    "  b3_1 = bias_variable([256])\n",
    "  conv3_1 = conv2d( conv2_2, W3_1, 1 ) + b3_1\n",
    "  conv3_1 = tf.nn.relu(conv3_1)\n",
    "\n",
    "  W3_2 = weight_variable([3,3,256,256])\n",
    "  b3_2 = bias_variable([256])\n",
    "  conv3_2 = conv2d( conv3_1, W3_2, 1 ) + b3_2\n",
    "  conv3_2 = tf.nn.relu(conv3_2)\n",
    "\n",
    "  W3_3 = weight_variable([3,3,256,256])\n",
    "  b3_3 = bias_variable([256])\n",
    "  conv3_3 = conv2d( conv3_2, W3_3, 2 ) + b3_3\n",
    "  conv3_3 = tf.nn.relu(conv3_3)\n",
    "\n",
    "  conv3_3 = tf.contrib.layers.batch_norm(conv3_3)\n",
    "\n",
    "  W4_1 = weight_variable([3,3,256,512])\n",
    "  b4_1 = bias_variable([512])\n",
    "  conv4_1 = conv2d( conv3_3, W4_1, 1 ) + b4_1\n",
    "  conv4_1 = tf.nn.relu(conv4_1)\n",
    "\n",
    "  W4_2 = weight_variable([3,3,512,512])\n",
    "  b4_2 = bias_variable([512])\n",
    "  conv4_2 = conv2d( conv4_1, W4_2, 1 ) + b4_2\n",
    "  conv4_2 = tf.nn.relu(conv4_2)\n",
    "\n",
    "  W4_3 = weight_variable([3,3,512,512])\n",
    "  b4_3 = bias_variable([512])\n",
    "  conv4_3 = conv2d( conv4_2, W4_3, 1 ) + b4_3\n",
    "  conv4_3 = tf.nn.relu(conv4_3)\n",
    "\n",
    "  conv4_3 = tf.contrib.layers.batch_norm(conv4_3)\n",
    "\n",
    "  W5_1 = weight_variable([3,3,512,512])\n",
    "  b5_1 = bias_variable([512])\n",
    "  conv5_1 = tf.nn.atrous_conv2d( conv4_3, W5_1, 2, padding = 'SAME') + b5_1\n",
    "  conv5_1 = tf.nn.relu(conv5_1)\n",
    "\n",
    "  W5_2 = weight_variable([3,3,512,512])\n",
    "  b5_2 = bias_variable([512])\n",
    "  conv5_2 = tf.nn.atrous_conv2d( conv5_1, W5_2, 2, padding = 'SAME') + b5_2\n",
    "  conv5_2 = tf.nn.relu(conv5_2)\n",
    "\n",
    "  W5_3 = weight_variable([3,3,512,512])\n",
    "  b5_3 = bias_variable([512])\n",
    "  conv5_3 = tf.nn.atrous_conv2d( conv5_2, W5_3, 2, padding = 'SAME') + b5_3\n",
    "  conv5_3 = tf.nn.relu(conv5_3)\n",
    "\n",
    "  conv5_3 = tf.contrib.layers.batch_norm(conv5_3)\n",
    "\n",
    "  W6_1 = weight_variable([3,3,512,512])\n",
    "  b6_1 = bias_variable([512])\n",
    "  conv6_1 = tf.nn.atrous_conv2d( conv5_3, W6_1, 2, padding = 'SAME') + b6_1\n",
    "  conv6_1 = tf.nn.relu(conv6_1)\n",
    "\n",
    "  W6_2 = weight_variable([3,3,512,512])\n",
    "  b6_2 = bias_variable([512])\n",
    "  conv6_2 = tf.nn.atrous_conv2d( conv6_1, W6_2, 2, padding = 'SAME') + b6_2\n",
    "  conv6_2 = tf.nn.relu(conv6_2)\n",
    "  \n",
    "  W6_3 = weight_variable([3,3,512,512])\n",
    "  b6_3 = bias_variable([512])\n",
    "  conv6_3 = tf.nn.atrous_conv2d( conv6_2, W6_3, 2, padding = 'SAME') + b6_3\n",
    "  conv6_3 = tf.nn.relu(conv6_3)\n",
    "\n",
    "  conv6_3 = tf.contrib.layers.batch_norm(conv6_3)\n",
    "\n",
    "  W7_1 = weight_variable([3,3,512,256])\n",
    "  b7_1 = bias_variable([256])\n",
    "  conv7_1 = conv2d( conv6_3, W7_1, 1 ) + b7_1\n",
    "  conv7_1 = tf.nn.relu(conv7_1)\n",
    "\n",
    "  W7_2 = weight_variable([3,3,256,256])\n",
    "  b7_2 = bias_variable([256])\n",
    "  conv7_2 = conv2d( conv7_1, W7_2, 1 ) + b7_2\n",
    "  conv7_2 = tf.nn.relu(conv7_2)\n",
    "\n",
    "  W7_3 = weight_variable([3,3,256,256])\n",
    "  b7_3 = bias_variable([256])\n",
    "  conv7_3 = conv2d( conv7_2, W7_3, 1 ) + b7_3\n",
    "  conv7_3 = tf.nn.relu(conv7_3)\n",
    "\n",
    "  conv7_3 = tf.contrib.layers.batch_norm(conv7_3)\n",
    "\n",
    "  conv7_3 = tf.image.resize_images(conv7_3, [64,64])\n",
    "\n",
    "  W8_1 = weight_variable([4,4,256,128])\n",
    "  b8_1 = bias_variable([128])\n",
    "  conv8_1 = conv2d( conv7_3, W8_1, 1 ) + b8_1\n",
    "  conv8_1 = tf.nn.relu(conv8_1)\n",
    "\n",
    "  W8_2 = weight_variable([3,3,128,128])\n",
    "  b8_2 = bias_variable([128])\n",
    "  conv8_2 = conv2d( conv8_1, W8_2, 1 ) + b8_2\n",
    "  conv8_2 = tf.nn.relu(conv8_2)\n",
    "\n",
    "  W8_3 = weight_variable([3,3,128,128])\n",
    "  b8_3 = bias_variable([128])\n",
    "  conv8_3 = conv2d( conv8_2, W8_3, 1 ) + b8_3\n",
    "  conv8_3 = tf.nn.relu(conv8_3)\n",
    "\n",
    "  W_ab = weight_variable([1,1,128,313])\n",
    "  b_ab = bias_variable([313])\n",
    "  conv_ab = conv2d( conv8_3, W_ab, 1 ) + b_ab\n",
    "  output = tf.nn.relu(conv_ab)\n",
    "  \n",
    "  return image_, output_, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(output, output_):\n",
    "  loss = tf.nn.softmax_cross_entropy_with_logits( output,  output_ )\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction( output ):\n",
    "  prediction = tf.nn.softmax( output )\n",
    "  return tf.image.resize_images( prediction, [256,256] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256, 256, 64)\n",
      "(1, 128, 128, 64)\n",
      "(1, 128, 128, 128)\n",
      "(1, 64, 64, 128)\n",
      "(1, 64, 64, 256)\n",
      "(1, 64, 64, 256)\n",
      "(1, 32, 32, 256)\n",
      "(1, 32, 32, 512)\n",
      "(1, 32, 32, 512)\n",
      "(1, 32, 32, 512)\n",
      "(1, 32, 32, 512)\n",
      "(1, 32, 32, 512)\n",
      "(1, 32, 32, 512)\n",
      "(1, 32, 32, 512)\n",
      "(1, 32, 32, 512)\n",
      "(1, 32, 32, 512)\n",
      "(1, 32, 32, 256)\n",
      "(1, 32, 32, 256)\n",
      "(1, 32, 32, 256)\n",
      "(1, 64, 64, 128)\n",
      "(1, 64, 64, 128)\n",
      "(1, 64, 64, 128)\n",
      "(1, 64, 64, 313)\n"
     ]
    }
   ],
   "source": [
    "image_, output_, output = setup_tensorflow_graph(1)\n",
    "loss = loss_function(output, output_)\n",
    "prediction = get_prediction(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(256), Dimension(256), Dimension(313)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "sess.run( train_step, feed_dict = {image_ : np.ones((1,256,256,1)), output_ : np.ones((1,64,64,313))})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
